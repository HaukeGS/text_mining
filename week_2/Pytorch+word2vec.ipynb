{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7102062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b91295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e9c138",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e2084",
   "metadata": {},
   "source": [
    "#1. The only datatype the model will understand is tensor (multi dimensional matrix)\n",
    "\n",
    "#2. You will have to convert the feature vector to a tensor \n",
    "\n",
    "#3. The entries in the tensor could be float, int,....\n",
    "\n",
    "#4. https://pytorch.org/docs/stable/tensors.html (lists all the available types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "038120e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general way of creating tensor - torch.tensor(<list/numpy array>, dtype=<dtype>)\n",
    "x = torch.tensor([1,2,3,4])\n",
    "y = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "z = torch.FloatTensor([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8604d6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f9c3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71c339f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "x_t = torch.tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cad91b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "999053dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can get back numpy array back from tensor\n",
    "x_n = x_t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea4c369c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a25a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to numpy there are different ways of creating tensors\n",
    "x = torch.ones((1,8))\n",
    "y = torch.zeros((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b43536b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7896dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar way of accessing elements as numpy\n",
    "y[0][0] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e10354e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0,0] # this also works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "987e9f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshaping matrix\n",
    "# We want to reshape the matrix of form 3x3 to 1x9\n",
    "z = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42dc7516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = z.reshape(1,9)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7356f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we know how to create and manipulate tensors, we move onto the second part which is calculating gradients\n",
    "# You donot need to calculate gradients from scratch..\n",
    "# Autograd module in Pytorch does it for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3476feec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c702ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((1,7),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca2965d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.sum(x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9e884f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # calculates gradient of y wrt all variables (x in this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1115b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad) # dy/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248fda4d",
   "metadata": {},
   "source": [
    "# Creating a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48c83357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will implement the word2vec algorthm which is a simple feed-forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd29f05f",
   "metadata": {},
   "source": [
    "<img src=\"word2vec.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e896d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ab6c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec(nn.Module):\n",
    "    def __init__(self, v_size, dimension):\n",
    "        super().__init__()\n",
    "        self.v_size = v_size # vocabulary size\n",
    "        self.dim = dimension # dimension of embedding\n",
    "        self.W_e = nn.Parameter(torch.rand((self.v_size, self.dim)))\n",
    "        self.W_c = nn.Parameter(torch.rand((self.dim, self.v_size)))\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, t_w):\n",
    "        e_w = self.W_e[t_w]\n",
    "        out = torch.matmul(e_w, self.W_c)\n",
    "        out = self.softmax(out)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba9e79",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39175161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6761452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now generate center/target-context word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d8b6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open('abc_news/abcnews-date-text.csv', 'r') as fs:\n",
    "    for line in fs:\n",
    "        #print(line)\n",
    "        ind = line.find(\",\")                   \n",
    "        text = line[ind+1:]\n",
    "        sentences.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93f25cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "puncts = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0f945c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "s_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d87eca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sentences = []\n",
    "i = 0\n",
    "for sent in sentences:\n",
    "    words = word_tokenize(sent)\n",
    "    words = [w.lower() for w in word_tokenize(sent) if w not in puncts and w.lower() not in s_words]\n",
    "    n_sentences.append(words)\n",
    "    i+=1\n",
    "    if i==10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bf995e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aba', 'decides', 'community', 'broadcasting', 'licence']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3564cb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03d188b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b2236ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bd3d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in n_sentences:\n",
    "    for w in sent:\n",
    "        word_count[w]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29664778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9939"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "842fa517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the words that occurred less than 5 times\n",
    "word2index = {}\n",
    "index2word = {}\n",
    "i=0\n",
    "for w in word_count:\n",
    "    if word_count[w]>5:\n",
    "        word2index[w] = i\n",
    "        index2word[i] = w\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16ce8e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1986, 1986)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index), len(index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22954f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we remove the words which appeared less than 5 times in the corpus\n",
    "all_sentences = []\n",
    "for sent in n_sentences:\n",
    "    s = [w for w in sent if w in word2index]\n",
    "    all_sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "420cb5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d641545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 97242.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# we consider a context window of 2\n",
    "window_size = 2\n",
    "word_context_pairs = []\n",
    "for sent in tqdm(all_sentences):\n",
    "    #print(sent)\n",
    "    for i, word in enumerate(sent):\n",
    "        t_word = word2index[word]\n",
    "        for j in range(1,window_size+1):\n",
    "            if i-j>=0:\n",
    "                c_word = word2index[sent[i-j]]\n",
    "                word_context_pairs.append((t_word, c_word))\n",
    "            if i+j<len(sent):\n",
    "                c_word = word2index[sent[i+j]]\n",
    "                word_context_pairs.append((t_word, c_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f57a46eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95444"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_context_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0868f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36e8cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(batch_size=64):\n",
    "    for i in range(0, len(word_context_pairs), batch_size):\n",
    "        yield word_context_pairs[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d38c61ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 0), (2, 3), (2, 4), (3, 2), (3, 4), (3, 5), (4, 3), (4, 5), (4, 2), (5, 4), (5, 3), (6, 7), (6, 8), (7, 6), (7, 8), (7, 9), (8, 7), (8, 9), (8, 6), (9, 8), (9, 7), (10, 11), (10, 12), (11, 10), (11, 12), (11, 13), (12, 11), (12, 13), (12, 10), (12, 14), (13, 12), (13, 14), (13, 11), (13, 15), (14, 13), (14, 15), (14, 12), (14, 16), (15, 14), (15, 16), (15, 13), (16, 15), (16, 14), (10, 11), (10, 14), (11, 10), (11, 14), (11, 17), (14, 11), (14, 17), (14, 10), (14, 18), (17, 14), (17, 18), (17, 11), (18, 17), (18, 14), (19, 20), (19, 21), (20, 19), (20, 21), (21, 20), (21, 19)]\n"
     ]
    }
   ],
   "source": [
    "for batch in get_batches():\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8b950b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = list(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a093ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD # for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50057dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1492it [00:00, 2644.09it/s]\n",
      "1492it [00:00, 2533.31it/s]\n",
      "1492it [00:00, 2576.79it/s]\n",
      "1492it [00:00, 2568.53it/s]\n",
      "1492it [00:00, 2506.17it/s]\n"
     ]
    }
   ],
   "source": [
    "model = Word2vec(len(word2index), 100)\n",
    "epochs = 5\n",
    "optimizer = SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss() # negative log-likelihood loss\n",
    "for e in range(epochs):\n",
    "    for i, batch in tqdm(enumerate(get_batches())):\n",
    "        X,y = list(zip(*batch))\n",
    "        X = torch.tensor(X)\n",
    "        y = torch.tensor(y)\n",
    "        out = model(X)\n",
    "        loss = criterion(out, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60e372d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model, word):\n",
    "    index = word2index[word]\n",
    "    return model.W_e[index].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2fae074e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'licence'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9b3db40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5728, 0.6929, 0.3726, 0.4095, 0.7465, 0.4397, 0.3540, 0.1893, 0.2499,\n",
       "        0.1384, 0.1784, 0.9345, 0.5357, 0.7073, 0.0518, 0.8645, 0.8236, 0.6776,\n",
       "        0.9739, 0.8267, 0.8614, 0.3344, 0.1896, 0.6776, 0.1808, 0.9190, 0.6413,\n",
       "        0.7563, 0.8817, 0.1728, 0.5030, 0.0912, 0.7279, 0.1359, 0.4017, 0.7962,\n",
       "        0.6545, 0.0746, 0.9268, 0.1942, 0.5248, 0.7611, 0.1689, 0.7647, 0.7647,\n",
       "        0.9167, 0.0886, 0.9614, 0.3198, 0.2808, 0.9089, 0.4913, 0.6085, 0.7777,\n",
       "        0.1603, 0.1803, 0.4515, 0.7839, 0.3217, 0.6338, 0.9672, 0.8717, 0.5108,\n",
       "        0.0908, 0.4319, 0.0722, 0.4457, 0.8063, 0.1259, 0.7349, 0.9336, 0.4338,\n",
       "        0.2817, 0.6508, 0.2732, 0.5957, 0.7631, 0.1359, 0.9362, 0.3379, 0.0844,\n",
       "        0.5994, 0.9929, 0.6280, 0.5156, 0.1353, 0.6112, 0.5196, 0.8100, 0.4631,\n",
       "        0.6538, 0.7504, 0.7148, 0.6516, 0.8599, 0.2443, 0.8138, 0.0674, 0.6872,\n",
       "        0.8376])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_embeddings(model, 'licence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6551367",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1846c179",
   "metadata": {},
   "source": [
    "Download the Game of Thrones Corpus. Perform pre-processing to remove stopwords and punctuations.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa8cf724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76f64938",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "punctuations = string.punctuation\n",
    "punctuations = punctuations + '”“’'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26c2ca65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This edition contains the complete text of the original hardcover edition.\\n',\n",
       " '\\n',\n",
       " 'NOT ONE WORD HAS BEEN OMITTED.\\n',\n",
       " '\\n',\n",
       " 'A CLASH OF KINGS\\n',\n",
       " '\\n',\n",
       " 'A Bantam Spectra Book\\n',\n",
       " '\\n',\n",
       " 'PUBLISHING HISTORY\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_text = []\n",
    "for filename in os.listdir('GOT'):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join('GOT', filename)) as file:\n",
    "            for line in file:\n",
    "                whole_text.append(line)\n",
    "whole_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a07b350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['edition',\n",
       "  'contains',\n",
       "  'complete',\n",
       "  'text',\n",
       "  'original',\n",
       "  'hardcover',\n",
       "  'edition'],\n",
       " [],\n",
       " ['one', 'word', 'omitted'],\n",
       " [],\n",
       " ['clash', 'kings']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_lines = []\n",
    "for line in whole_text:\n",
    "    words = [w.lower() for w in word_tokenize(line) if w not in punctuations and w.lower() not in s_words]\n",
    "    tokenized_lines.append(words)\n",
    "tokenized_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6117763",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_text.txt', 'w') as file:\n",
    "    for line in tokenized_lines:\n",
    "        if not line:\n",
    "            continue\n",
    "        file.write(' '.join(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ad8c0",
   "metadata": {},
   "source": [
    "Obtain the word2index and index2word maps for all the unique words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a529cf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27848\n"
     ]
    }
   ],
   "source": [
    "word_count = Counter()\n",
    "for line in tokenized_lines:\n",
    "    for word in line:\n",
    "        word_count[word] += 1\n",
    "print(len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3a0f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "index2word = {}\n",
    "i = 0\n",
    "for w in word_count:\n",
    "    if word_count[w]>=5:\n",
    "        word2index[w] = i\n",
    "        index2word[i] = w\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8547d444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['edition', 'complete', 'edition'], [], ['one', 'word'], [], ['clash', 'kings']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = []\n",
    "for line in tokenized_lines:\n",
    "    l = [w for w in line if w in word2index]\n",
    "    cleaned_text.append(l)\n",
    "print(cleaned_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "209a44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_text.txt', 'w') as file:\n",
    "    for line in cleaned_text:\n",
    "        if not line:\n",
    "            continue\n",
    "        file.write(' '.join(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096cef28",
   "metadata": {},
   "source": [
    "Train word2vec algorithm on this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e95b820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92493/92493 [00:01<00:00, 85553.83it/s] \n"
     ]
    }
   ],
   "source": [
    "# we consider a context window of 2\n",
    "window_size = 2\n",
    "word_context_pairs = []\n",
    "for line in tqdm(cleaned_text):\n",
    "    for i, word in enumerate(line):\n",
    "        t_word = word2index[word]\n",
    "        for j in range(1,window_size+1):\n",
    "            if i-j>=0:\n",
    "                c_word = word2index[line[i-j]]\n",
    "                word_context_pairs.append((t_word, c_word))\n",
    "            if i+j<len(line):\n",
    "                c_word = word2index[line[i+j]]\n",
    "                word_context_pairs.append((t_word, c_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aac9b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(batch_size=64):\n",
    "    for i in range(0, len(word_context_pairs), batch_size):\n",
    "        yield word_context_pairs[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02cfef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51502it [00:27, 1865.33it/s]\n",
      "51502it [00:27, 1858.92it/s]\n",
      "51502it [00:26, 1934.79it/s]\n",
      "51502it [00:27, 1889.08it/s]\n",
      "51502it [00:26, 1916.63it/s]\n"
     ]
    }
   ],
   "source": [
    "model = Word2vec(len(word2index), 100).to(device)\n",
    "epochs = 5\n",
    "optimizer = SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss() # negative log-likelihood loss\n",
    "for e in range(epochs):\n",
    "    for i, batch in tqdm(enumerate(get_batches())):\n",
    "        X,y = list(zip(*batch))\n",
    "        X = torch.tensor(X).to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "        out = model(X)\n",
    "        loss = criterion(out, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd27420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model, word):\n",
    "    index = word2index[word]\n",
    "    return model.W_e[index].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0db0f41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3638, 0.3721, 0.3662, 0.2150, 0.3640, 0.5664, 0.9018, 0.2554, 0.9234,\n",
      "        0.1999, 0.5275, 0.1165, 0.6489, 0.3467, 0.6048, 0.0961, 0.0184, 0.7646,\n",
      "        0.2026, 0.9545, 0.9708, 0.1207, 0.8303, 0.9559, 0.8189, 0.3335, 0.1994,\n",
      "        0.6889, 0.8779, 0.5850, 0.6059, 0.3932, 0.9026, 0.1976, 0.8631, 0.9511,\n",
      "        0.3445, 0.7543, 0.9619, 0.5030, 0.6944, 0.9572, 0.2966, 0.2563, 0.6139,\n",
      "        0.1872, 0.2527, 0.9310, 0.7091, 0.6723, 0.7228, 0.0354, 0.2942, 0.4424,\n",
      "        0.7459, 0.8883, 0.2450, 0.8049, 0.0234, 0.5913, 0.7089, 0.4237, 0.9630,\n",
      "        0.7962, 0.9481, 0.2312, 0.8720, 0.2341, 0.2099, 0.3356, 0.7303, 0.0973,\n",
      "        0.3383, 0.8200, 0.7255, 0.5373, 0.7961, 0.9242, 0.1236, 0.8007, 0.4954,\n",
      "        0.6857, 0.2211, 0.5509, 0.6727, 0.8436, 0.8034, 0.2699, 0.9625, 0.9039,\n",
      "        0.0415, 0.1289, 0.2299, 0.8344, 0.3997, 0.0583, 0.2278, 0.1008, 0.1953,\n",
      "        0.1639], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(generate_embeddings(model, 'sansa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1369c",
   "metadata": {},
   "source": [
    "Write a function to compute similarity between embeddings of two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a258dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dcf3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb83003b",
   "metadata": {},
   "source": [
    "Compute similarity between two characters in GOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5a8a8bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.77543]]\n"
     ]
    }
   ],
   "source": [
    "embedding_1 = generate_embeddings(model, 'daenerys').to('cpu')\n",
    "embedding_2 = generate_embeddings(model, 'tyrion').to('cpu')\n",
    "print(cosine_similarity(embedding_1.reshape(1, -1), embedding_2.reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815bea20",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_mining_scikit-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
