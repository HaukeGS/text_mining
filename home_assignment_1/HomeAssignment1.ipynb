{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment 1\n",
    "Due by 8th May, 2024 at 23:59 CEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students:\n",
    "* Hauke Sch√ºle, 10004972\n",
    "* Jannik Jahn, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Basics\n",
    "\n",
    "We want to create a 2 layer NN, which means we want to calculate  $y = W_2 * ReLU(W_1 * x + b_1) + b_2$\n",
    "\n",
    "Complete the TODOs below to create such a NN.\n",
    "\n",
    "Since you will be needing to compute the gradients w.r.t. all parameters, you may look into online resources for help. Please cite or link any online recources you do use.\n",
    "\n",
    "You are allowed to change any existing parts, however the code has to remain easy to understand and well documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    \n",
    "    Parameters:\n",
    "        x (np.ndarray): numpy array with shape (m, n) where m is the number of dimensions and n is the number of points\n",
    "        \n",
    "    Returns:\n",
    "        x' (np.ndarray): return value of the pointwise ReLU application\n",
    "    \"\"\"\n",
    "    r = x.copy()\n",
    "    for element in np.nditer(r, op_flags=['readwrite']):\n",
    "        if element < 0:\n",
    "            element[...] = 0\n",
    "        else:\n",
    "            element[...] = element\n",
    "    return r\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    r_grad = x.copy()\n",
    "    for element in np.nditer(r_grad, op_flags=['readwrite']):\n",
    "        if element <= 0:\n",
    "            element[...] = 0\n",
    "        else:\n",
    "            element[...] = 1\n",
    "    return r_grad\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    Parameters:\n",
    "        x (np.ndarray): numpy array with shape (m, n) where m is the number of dimensions and n is the number of points\n",
    "    Returns:\n",
    "        x' (np.ndarray): return value of the pointwise sigmoid application\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumPyNeuralNet:\n",
    "    \n",
    "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        \n",
    "        self.W_1 = np.random.randn(dim_in, dim_hidden) # weight matrix for first layer\n",
    "        self.W_2 = np.random.randn(dim_hidden, dim_out) # weight matrix for second layer\n",
    "        self.b_1 = np.random.randn(1, dim_hidden) # bias for first layer, shape (1, n) for easy matrix addition\n",
    "        self.b_2 = np.random.randn(1, dim_out) # bias for second layer, shape (1, n) for easy matrix addition\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the output of the neural network for the given x.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input value numpy array\n",
    "        \n",
    "        Returns:\n",
    "            y (np.ndarray): predicted output for `x`\n",
    "        \"\"\"\n",
    "        if x.shape != (1, self.dim_in):\n",
    "            raise ValueError(f\"Input dimension {x.shape} does not match the expected input dimension {(1, self.dim_in)}\")\n",
    "        hidden = relu((x @ self.W_1) + self.b_1)\n",
    "        output = hidden @ self.W_2 + self.b_2\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the Mean-Squared Error and returns the gradients w.r.t. to the parameters.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input value numpy array with shape (self.dim_in, n)\n",
    "            y (np.ndarray): ground truth value numpy array with shape (self.dim_out, n)\n",
    "            \n",
    "        Returns:\n",
    "            loss (float): Mean-Squared-Error between predicted value on input points and ground truth value\n",
    "            W_1_grad (np.ndarray): gradient w.r.t W_1   \n",
    "            W_2_grad (np.ndarray): gradient w.r.t W_2  \n",
    "            b_1_grad (np.ndarray): gradient w.r.t b_1   \n",
    "            b_2_grad (np.ndarray): gradient w.r.t b_2   \n",
    "        \"\"\"\n",
    "        # make sure that the input and output dimensions are correct\n",
    "        if x.shape != (1, self.dim_in):\n",
    "            raise ValueError(f\"Input dimension {x.shape} does not match the expected input dimension {(1, self.dim_in)}\")\n",
    "        if y.shape != (1, self.dim_out):\n",
    "            raise ValueError(f\"Ground truth dimension {y.shape} does not match the expected output dimension {(1, self.dim_out)}\")\n",
    "        y_pred = self.predict(x)\n",
    "\n",
    "        if y_pred.shape != y.shape:\n",
    "            raise ValueError(f\"Predicted shape {y_pred.shape} does not match the expected shape {y.shape}\")\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = 0.5 * np.mean(np.square(y_pred - y)) # Mean Squared Error with 1/2 factor for easier gradient calculation\n",
    "        \n",
    "        # calculate the gradients\n",
    "        l_grad = y_pred - y # gradient of the loss w.r.t. the output\n",
    "        h = x @ self.W_1 + self.b_1\n",
    "        r = relu(h)\n",
    "        r_grad = relu_grad(h)\n",
    "\n",
    "        b_2_grad = np.array(l_grad)\n",
    "        W_2_grad = r.T @ l_grad\n",
    "        b_1_grad = l_grad @ self.W_2.T * r_grad\n",
    "        W_1_grad = x.T @ (l_grad @ self.W_2.T * r_grad)\n",
    "\n",
    "        return loss, W_1_grad, W_2_grad, b_1_grad, b_2_grad\n",
    "\n",
    "    \n",
    "    def update(self, W_1_grad, W_2_grad, b_1_grad, b_2_grad, lr=0.001):\n",
    "        \"\"\"\n",
    "        Update the parameters of the neural network with the gradients.\n",
    "        \n",
    "        Parameters:\n",
    "            W_1_grad (np.ndarray): gradient w.r.t W_1\n",
    "            W_2_grad (np.ndarray): gradient w.r.t W_2\n",
    "            b_1_grad (np.ndarray): gradient w.r.t b_1\n",
    "            b_2_grad (np.ndarray): gradient w.r.t b_2\n",
    "            lr (float): learning rate, default: 0.001\n",
    "        \"\"\"\n",
    "        self.W_1 -= lr * W_1_grad\n",
    "        self.W_2 -= lr * W_2_grad\n",
    "        self.b_1 -= lr * b_1_grad\n",
    "        self.b_2 -= lr * b_2_grad\n",
    "\n",
    "         \n",
    "    def train(self, x, y, lr=0.001, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train the neural network with gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input values\n",
    "            y (np.ndarray): ground truth values\n",
    "            lr (float): learning rate, default: 0.001\n",
    "            epochs (int): number of epochs to train, default: 1000\n",
    "            \n",
    "        Returns:\n",
    "            loss (float): Return the loss achieved after all epochs\n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        # for every epoch\n",
    "        for i in tqdm(range(epochs), desc=f\"Epoch: {i+1}/{epochs}\"):\n",
    "            # for every sample in the dataset\n",
    "            loss_eopch = [] # plot the mean loss for every epoch\n",
    "            for j in range(x.shape[0]): # x shape is to be expected as (n, 1, dim_in). n samples in the dataset with the extra dimension for easier matrix multiplication\n",
    "                x_j = x[j]\n",
    "                y_j = y[j]\n",
    "                loss, W_1_grad, W_2_grad, b_1_grad, b_2_grad = self.loss(x_j, y_j)\n",
    "                self.update(W_1_grad, W_2_grad, b_1_grad, b_2_grad, lr)\n",
    "                loss_eopch.append(loss)\n",
    "            loss_history.append(np.mean(loss_eopch))\n",
    "            lr = lr * 0.99\n",
    "        plt.plot(loss_history)\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.title(\"Average Loss over epochs\")\n",
    "        plt.show()\n",
    "        return loss_history[-1] # return the final loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test the model created above on the simple function y = x^2\n",
    "\n",
    "model = NumPyNeuralNet(1, 20, 1)\n",
    "\n",
    "# Create a randomly distributed array of 1000 values\n",
    "x_train = 4 * np.random.randn(1000, 1, 1)\n",
    "# Create ground truth by calculating x*x\n",
    "y_train = np.expand_dims([[x_train[i][0][0] * x_train[i][0][0]] for i in range(x_train.shape[0])], axis=1)\n",
    "\n",
    "loss = model.train(x_train, y_train, lr=0.001, epochs=10)\n",
    "print(f\"Final loss: {loss}\")\n",
    "\n",
    "# 1000 samples with 1000 epochs work, but *10 is too large values. I suspect that the weights are too large\n",
    "# and then we get overflow errors. But with *4 it works quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50, 1, 4)\n",
      "y_train shape: (50, 1, 2)\n",
      "y_train: [[[ 9.86761498  0.41139627]]\n",
      "\n",
      " [[-3.57459724 -5.67038828]]\n",
      "\n",
      " [[ 1.63627442 -6.79012873]]\n",
      "\n",
      " [[-2.6620367  -6.44688524]]\n",
      "\n",
      " [[-7.26176289 -3.68776256]]\n",
      "\n",
      " [[ 3.28752578 -1.11893871]]\n",
      "\n",
      " [[-1.05915155  7.41607126]]\n",
      "\n",
      " [[-0.70271516 -0.34216641]]\n",
      "\n",
      " [[ 7.20820917 -5.55165751]]\n",
      "\n",
      " [[-6.71228254  7.1622934 ]]\n",
      "\n",
      " [[-1.11519269 -3.73707296]]\n",
      "\n",
      " [[ 4.42213936 -8.42146687]]\n",
      "\n",
      " [[-3.10575554 -4.99702078]]\n",
      "\n",
      " [[-0.81222526  5.02653819]]\n",
      "\n",
      " [[-5.60092236 -4.88787724]]\n",
      "\n",
      " [[ 5.71549451 -5.51855399]]\n",
      "\n",
      " [[-2.35191106 -2.25030231]]\n",
      "\n",
      " [[ 1.94991008  1.1513359 ]]\n",
      "\n",
      " [[-3.32829909 -0.01777778]]\n",
      "\n",
      " [[-6.67306879 -5.50795332]]\n",
      "\n",
      " [[-8.32737787 -5.27800556]]\n",
      "\n",
      " [[-7.01246917 -1.8172599 ]]\n",
      "\n",
      " [[ 2.42694812  2.44307951]]\n",
      "\n",
      " [[ 8.76392668  8.32993708]]\n",
      "\n",
      " [[ 7.81745066  2.66339982]]\n",
      "\n",
      " [[ 5.01344491  5.19567501]]\n",
      "\n",
      " [[ 0.61258122  0.33101458]]\n",
      "\n",
      " [[-7.06694616  9.22776702]]\n",
      "\n",
      " [[ 0.46222087  2.68451607]]\n",
      "\n",
      " [[-0.10413867 -3.17328228]]\n",
      "\n",
      " [[-5.8676297   2.64212525]]\n",
      "\n",
      " [[ 5.23972356  4.67080204]]\n",
      "\n",
      " [[ 1.89604156  3.67901644]]\n",
      "\n",
      " [[-5.6907782   4.86750224]]\n",
      "\n",
      " [[-5.26913117  4.36533162]]\n",
      "\n",
      " [[10.55293241  0.87780702]]\n",
      "\n",
      " [[-1.87884193 -4.85558481]]\n",
      "\n",
      " [[-1.54091971 -8.99727465]]\n",
      "\n",
      " [[-9.7838554  -2.07672737]]\n",
      "\n",
      " [[ 2.9508367  -3.51883949]]\n",
      "\n",
      " [[ 4.11028898  3.23880301]]\n",
      "\n",
      " [[ 9.5294714   4.2769033 ]]\n",
      "\n",
      " [[ 6.99121562  5.81378019]]\n",
      "\n",
      " [[ 2.56500833 -8.76833863]]\n",
      "\n",
      " [[-4.88282432  2.72895707]]\n",
      "\n",
      " [[ 1.30303147  2.83807093]]\n",
      "\n",
      " [[-1.76186803 -4.3423601 ]]\n",
      "\n",
      " [[ 1.62937686 -8.1326135 ]]\n",
      "\n",
      " [[ 0.93783166 -8.60100937]]\n",
      "\n",
      " [[ 0.5995148   0.11369055]]]\n",
      "x_test [[ 4.91489282  4.66451654  3.24050604 -0.24384169]]\n",
      "Predicted: [[10.59890704  3.93827745]]\n",
      "Expected: [[9.57940936 2.99666435]]\n",
      "Final loss: 0.1472625869307302\n"
     ]
    }
   ],
   "source": [
    "# test the model on more than one input dimension and with reasonable number of epochs and samples on a simple function\n",
    "\n",
    "model = NumPyNeuralNet(4, 20, 2)\n",
    "\n",
    "x_train = 4 * np.random.randn(50, 1, 4)\n",
    "y_train = np.expand_dims([[x_train[i][0][0] + x_train[i][0][1], x_train[i][0][2] + x_train[i][0][3]] for i in range(x_train.shape[0])], axis=1)\n",
    "\n",
    "loss = model.train(x_train, y_train, lr=0.001, epochs=30)\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_train: {y_train}\")\n",
    "x_test = 2 * np.random.randn(1, 4)\n",
    "y_test = np.array([[x_test[0][0] + x_test[0][1], x_test[0][2] + x_test[0][3]]])\n",
    "print(f\"x_test {x_test}\")\n",
    "print(f\"Predicted: {model.predict(x_test)}\")\n",
    "print(f\"Expected: {y_test}\")\n",
    "print(f\"Final loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "### Intrinsic evaluation of embeddings\n",
    "Word similarity task is often used as an intrinsic evaluation criteria. In the dataset file you will find a list of word pairs with their similarity scores as judged by humans. The task would be to judge how well are the word vectors aligned to human judgement. We will use word2vec embedding vectors trained on the google news corpus. (Ignore the pairs where at least one the words is absent in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which takes as input two words and computes the cosine similarity between them.\n",
    "You do not need to implement the cosine similarity calculation from scratch. Feel free to use any Python library.\n",
    "Remeber to ignore any pairs where at least one word is absent in the corpus. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two words.\n",
    "    \n",
    "    Parameters:\n",
    "        word1 (str): first word\n",
    "        word2 (str): second word\n",
    "        \n",
    "    Returns:\n",
    "        sim (float): cosine similarity between the two words\n",
    "    \"\"\"\n",
    "    if word1 not in wv.key_to_index or word2 not in wv.key_to_index:\n",
    "        print(f\"One of the words '{word1}' or '{word2}' is not in the vocabulary.\")\n",
    "        return None\n",
    "    sim = cosine_similarity(wv[word1].reshape(1, -1), wv[word2].reshape(1, -1))\n",
    "    return sim[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarity between all the word pairs in the list and sort them based on the similarity scores. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"wordsim353_dataset.csv\")\n",
    "df['similarity_score'] = df.apply(lambda row: similarity(row['word_1'], row['word_2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tiger', 'tiger'), ('Harvard', 'Yale'), ('man', 'woman'), ('football', 'soccer'), ('mile', 'kilometer'), ('vodka', 'brandy'), ('journey', 'voyage'), ('football', 'basketball'), ('type', 'kind'), ('championship', 'tournament')]\n",
      "           word_1      word_2  human_mean  similarity_score\n",
      "2           tiger       tiger       10.00          1.000000\n",
      "209       Harvard        Yale        8.13          0.781770\n",
      "306           man       woman        8.30          0.766401\n",
      "42       football      soccer        9.03          0.731355\n",
      "170          mile   kilometer        8.66          0.725848\n",
      "59          vodka      brandy        8.13          0.688149\n",
      "68        journey      voyage        9.29          0.683085\n",
      "43       football  basketball        6.81          0.668247\n",
      "236          type        kind        8.97          0.666641\n",
      "296  championship  tournament        8.36          0.665532\n"
     ]
    }
   ],
   "source": [
    "df.sort_values(by='similarity_score', ascending=False, inplace=True)\n",
    "word_pairs_cosine_similarity = list(zip(df['word_1'].tolist(), df['word_2'].tolist()))\n",
    "print(word_pairs_cosine_similarity[:10])\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the word pairs in the list based on the human judgement scores. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tiger', 'tiger'), ('fuck', 'sex'), ('midday', 'noon'), ('journey', 'voyage'), ('dollar', 'buck'), ('money', 'cash'), ('coast', 'shore'), ('money', 'cash'), ('money', 'currency'), ('football', 'soccer')]\n",
      "       word_1    word_2  human_mean  similarity_score\n",
      "2       tiger     tiger       10.00          1.000000\n",
      "40       fuck       sex        9.44          0.223392\n",
      "73     midday      noon        9.29          0.552741\n",
      "68    journey    voyage        9.29          0.683085\n",
      "266    dollar      buck        9.22          0.256212\n",
      "31      money      cash        9.15          0.615122\n",
      "70      coast     shore        9.10          0.508367\n",
      "97      money      cash        9.08          0.615122\n",
      "98      money  currency        9.04          0.160101\n",
      "42   football    soccer        9.03          0.731355\n"
     ]
    }
   ],
   "source": [
    "df.sort_values(by='human_mean', ascending=False, inplace=True)\n",
    "word_pairs_human_mean = list(zip(df['word_1'].tolist(), df['word_2'].tolist()))\n",
    "print(word_pairs_human_mean[:10])\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute spearman rank correlation between the two ranked lists obtained in the previous two steps.\n",
    "You do not need to implement the spearman rank correlation calculation from scratch. Feel free to use any Python library. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len word_pairs_cosine_similarity: 353\n",
      "len word_pairs_human_mean: 353\n"
     ]
    }
   ],
   "source": [
    "print(f\"len word_pairs_cosine_similarity: {len(word_pairs_cosine_similarity)}\")\n",
    "print(f\"len word_pairs_human_mean: {len(word_pairs_human_mean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>human_mean</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>rank_cosine_similarity</th>\n",
       "      <th>rank_human_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>fuck</td>\n",
       "      <td>sex</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.223392</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>midday</td>\n",
       "      <td>noon</td>\n",
       "      <td>9.29</td>\n",
       "      <td>0.552741</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>journey</td>\n",
       "      <td>voyage</td>\n",
       "      <td>9.29</td>\n",
       "      <td>0.683085</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>dollar</td>\n",
       "      <td>buck</td>\n",
       "      <td>9.22</td>\n",
       "      <td>0.256212</td>\n",
       "      <td>173.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_1  word_2  human_mean  similarity_score  rank_cosine_similarity  \\\n",
       "2      tiger   tiger       10.00          1.000000                     1.0   \n",
       "40      fuck     sex        9.44          0.223392                   200.0   \n",
       "73    midday    noon        9.29          0.552741                    34.0   \n",
       "68   journey  voyage        9.29          0.683085                     7.0   \n",
       "266   dollar    buck        9.22          0.256212                   173.0   \n",
       "\n",
       "     rank_human_mean  \n",
       "2                1.0  \n",
       "40               2.0  \n",
       "73               3.5  \n",
       "68               3.5  \n",
       "266              5.0  "
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ranked = df.copy()\n",
    "df_ranked['rank_cosine_similarity'] = df_ranked['similarity_score'].rank(ascending=False)\n",
    "df_ranked['rank_human_mean'] = df_ranked['human_mean'].rank(ascending=False)\n",
    "df_ranked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.7000, p-value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Spearman correlation between the similarity score ranks and the human mean ranks\n",
    "correlation, p_value = stats.pearsonr(df_ranked['rank_cosine_similarity'], df_ranked['rank_human_mean'])\n",
    "print(f\"Spearman correlation: {correlation:.4f}, p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.6525, p-value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Spearman correlation between the similarity scores and human mean ratings\n",
    "correlation, p_value = stats.pearsonr(df['similarity_score'], df['human_mean'])\n",
    "print(f\"Spearman correlation: {correlation:.4f}, p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding based clasifier\n",
    "We will design a simple sentiment classifier based on the pre-trained word embeddings (google news).\n",
    "\n",
    "Each data point is a movie review and the sentiment could be either positive (1) or negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('sentiment_test_X.p', 'rb') as fs:\n",
    "    test_X = pickle.load(fs)\n",
    "\n",
    "len(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sentiment_test_y.p', 'rb') as fs:\n",
    "    test_y = pickle.load(fs)\n",
    "    \n",
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_train_X.p', 'rb') as fs:\n",
    "    train_X = pickle.load(fs)\n",
    "with open('sentiment_train_y.p', 'rb') as fs:\n",
    "    train_y = pickle.load(fs)\n",
    "with open('sentiment_val_X.p', 'rb') as fs:\n",
    "    val_X = pickle.load(fs)\n",
    "with open('sentiment_val_y.p', 'rb') as fs:\n",
    "    val_y = pickle.load(fs)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a review, compute its embedding by averaging over the embedding of its constituent words. Define a function which given a review as a list of words, generates its embeddings by averaging over the constituent word embeddings. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(review):\n",
    "    \"\"\"\n",
    "    Generate the embedding for a review.\n",
    "    \n",
    "    Parameters:\n",
    "        review (np.ndarray): review to generate the embedding for\n",
    "        \n",
    "    Returns:\n",
    "        embedding (np.ndarray): embedding of the review\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for word in review:\n",
    "        embedding = wv[word] if word in wv.key_to_index else None\n",
    "        if embedding is not None:\n",
    "            embeddings.append(embedding)\n",
    "    return np.mean(embeddings, axis=0) if len(embeddings) > 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feed-forward network class with pytorch. (Hyperparamter choice such as number of layers, hidden size is left to you) (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        # We tried two different architectures, one really deep and one with only two layers\n",
    "        # Both performed quite similar\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, int (hidden_dim/2)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(int (hidden_dim/2), hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim*2, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, int (hidden_dim/2)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(int (hidden_dim/2), output_dim),\n",
    "            nn.Sigmoid()\n",
    "            # nn.Linear(input_dim, hidden_dim),\n",
    "            # nn.LeakyReLU(0.2),\n",
    "            # nn.Linear(hidden_dim, output_dim),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset class for efficiently enumerating over the dataset. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sent_data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if len(X) != len(y):\n",
    "            raise ValueError(f\"X and y must have the same length. X: {len(X)}, y: {len(y)}\")\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for i in range(len(X)):\n",
    "            embedding = generate_embedding(X[i])\n",
    "            if embedding is not None:\n",
    "                self.data.append(embedding)\n",
    "                self.labels.append(y[i])\n",
    "        self.data = np.array(self.data)\n",
    "        self.labels = np.array(self.labels)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train function to train model. At the end of each epoch compute the validation accuracy and save the model with the best validation accuracy. (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"models\"\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "model_path = os.path.join(directory_path, \"sentiment_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test data.\n",
    "    \n",
    "    Parameters:\n",
    "        model (nn.Module): trained model\n",
    "        test_data (DataLoader): test data loader\n",
    "        \n",
    "    Returns:\n",
    "        accuracy (float): accuracy of the model on the test data\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_data:\n",
    "            outputs = model(inputs.float())\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.squeeze() == labels.float()).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"total number of samples validated/tested {total}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopt your code to incorporate mini-batch training\n",
    "# Use cross-entropy as your loss function\n",
    "def train(model, training_data_loader, validation_data_loader, optimizer, criterion, epochs=5):\n",
    "    model.train()\n",
    "    # keep track of the best accuracy\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        # keep track of the running loss\n",
    "        running_loss = 0.0\n",
    "        training_data_loader = tqdm(training_data_loader, desc=f\"Epoch: {epoch+1}/{epochs}\")\n",
    "        for i, (inputs, labels) in enumerate(training_data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.float())\n",
    "            outputs = outputs.squeeze()\n",
    "            if outputs.shape != labels.shape:\n",
    "                raise ValueError(f\"Output shape {outputs.shape} does not match label shape {labels.shape}\")\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            training_data_loader.set_postfix({\"loss\": running_loss / (i + 1)})\n",
    "        # evaluate the model on the validation set and save the model if the accuracy is better than the best accuracy\n",
    "        accuracy = evaluate(model, validation_data_loader)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Model saved in epoch {epoch+1} with validation accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "INPUT_DIMENSION = wv[0].shape[0]\n",
    "HIDDEN_DIMENSION = 1024\n",
    "OUTPUT_DIMENSION = 1\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets and dataloaders \n",
    "train_dataset = sent_data(train_X, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataset = sent_data(val_X, val_y)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataset = sent_data(test_X, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:03<00:00, 68.56it/s, loss=0.492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of samples validated/tested 872\n",
      "Model saved in epoch 1 with validation accuracy: 0.7993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:03<00:00, 69.20it/s, loss=0.417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of samples validated/tested 872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:03<00:00, 68.62it/s, loss=0.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of samples validated/tested 872\n",
      "Model saved in epoch 3 with validation accuracy: 0.8165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:03<00:00, 68.88it/s, loss=0.384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of samples validated/tested 872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:03<00:00, 69.15it/s, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of samples validated/tested 872\n",
      "total number of samples validated/tested 1821\n",
      "Test accuracy: 0.8199\n"
     ]
    }
   ],
   "source": [
    "# create model, optimizer and loss function with the hyperparameters\n",
    "# and train the model\n",
    "model = Classifier(input_dim=INPUT_DIMENSION, hidden_dim=HIDDEN_DIMENSION, output_dim=OUTPUT_DIMENSION)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCELoss()\n",
    "train(\n",
    "    model=model, \n",
    "    training_data_loader=train_loader, \n",
    "    validation_data_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion, \n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# evaluate the model on the test set\n",
    "model = Classifier(input_dim=INPUT_DIMENSION, hidden_dim=HIDDEN_DIMENSION, output_dim=OUTPUT_DIMENSION)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "test_accuracy = evaluate(model, test_loader)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the test set and report the test accuracy. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved the evaluate method up top to be able to run the whole notebook in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of samples validated/tested 1821\n",
      "Test accuracy: 0.8199\n"
     ]
    }
   ],
   "source": [
    "model = Classifier(input_dim=INPUT_DIMENSION, hidden_dim=HIDDEN_DIMENSION, output_dim=OUTPUT_DIMENSION)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "test_accuracy = evaluate(model, test_loader)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tm_ha1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
