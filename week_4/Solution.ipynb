{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d312eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "434dc1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce0fe20",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea94283",
   "metadata": {},
   "source": [
    "We will train a CNN based classifier for the sentiment classification task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e97bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b629eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sentiment/sentiment_train_X.p', 'rb') as fs:\n",
    "    train_data = pickle.load(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d106b622",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sentiment/sentiment_train_y.p', 'rb') as fs:\n",
    "    train_label = pickle.load(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9bb9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sentiment/sentiment_val_X.p', 'rb') as fs:\n",
    "    val_data = pickle.load(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8adaa9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sentiment/sentiment_val_y.p', 'rb') as fs:\n",
    "    val_label = pickle.load(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51569336",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sentiment/sentiment_test_X.p', 'rb') as fs:\n",
    "    test_data = pickle.load(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50a6e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sentiment/sentiment_test_y.p', 'rb') as fs:\n",
    "    test_label = pickle.load(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c014437",
   "metadata": {},
   "source": [
    "1) Considering all the unique words present in the training data as your vocabulary create a word2index mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "008e46ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533ce0b922534f479066a0e6a40bd6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word2index = {}\n",
    "ind = 1\n",
    "for i, sent in tqdm(enumerate(train_data)):\n",
    "    for word in sent:\n",
    "        if word in word2index:\n",
    "            continue\n",
    "        else:\n",
    "            word2index[word] = ind\n",
    "            ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51b5af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16273"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "118e580b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.309104046242776"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(sent) for sent in train_data])/len(train_data) # average length of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc2d0a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(sent) for sent in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429725b1",
   "metadata": {},
   "source": [
    "2) Write a function which takes as input an input takes and returns a list of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddc73f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = ['The', 'movie', 'is', 'good']\n",
    "# output = [10, 15, 2, 4]\n",
    "# Note that the vocabulary is only made of words in the train set which mean in the val and test set \n",
    "# you may encounter new words. You can ignore such words and make sure your function is capable of handling it.\n",
    "def text2ids(sent, max_len=30):\n",
    "    enc = [word2index[word] for word in sent if word in word2index]\n",
    "    if len(enc)<max_len:\n",
    "        return enc + [0 for _ in range(max_len - len(enc))]\n",
    "    else:\n",
    "        return enc[:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1999b35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[109,\n",
       " 232,\n",
       " 3,\n",
       " 134,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2ids([\"this\", \"movie\", \"is\", \"good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6672525",
   "metadata": {},
   "source": [
    "3) Write a pytorch dataset class for the sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c68ab861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06ed45f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        inp_text = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        input_ids = text2ids(inp_text) # execute preprocess code here\n",
    "        return torch.LongTensor(input_ids), torch.FloatTensor([label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb9b39",
   "metadata": {},
   "source": [
    "2) Design a CNN based text classifier mode. It should include a embedding module which should be initialized randomly and trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36259980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Convnet,self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=len(word2index)+1,embedding_dim=200,padding_idx=0)\n",
    "        self.conv_1 = nn.Conv1d(200,200,5) # 200 filters of size 5\n",
    "        self.conv_2 = nn.Conv1d(200,200,4) # 200 filters of size 4\n",
    "        self.conv_3 = nn.Conv1d(200,200,3) # 200 filters of size 4\n",
    "        self.fc_3 = nn.Linear(600,1000)\n",
    "        self.fc_4 = nn.Linear(1000,1)\n",
    "        self.relu = nn.ReLU() # Activation function\n",
    "        \n",
    "    def forward(self,inp):\n",
    "        inp = self.embed(inp)\n",
    "        inp = inp.transpose(2,1)\n",
    "        out_1 = self.conv_1(inp)\n",
    "        out_2 = self.conv_2(inp)\n",
    "        out_3 = self.conv_3(inp)\n",
    "        out_1 = self.relu(out_1)\n",
    "        out_2 = self.relu(out_2)\n",
    "        out_3 = self.relu(out_3)\n",
    "        out_1 = torch.max(out_1, dim=2)\n",
    "        out_2 = torch.max(out_2, dim=2)\n",
    "        out_3 = torch.max(out_3, dim=2)\n",
    "        #print(out_1[0].shape)\n",
    "        out = torch.cat((out_1[0], out_2[0], out_3[0]), dim=1)\n",
    "        out = self.fc_3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc_4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a44762",
   "metadata": {},
   "source": [
    "3) Train the model for 10 epochs. At the end of each epoch, compute validation accuracy and save the model with the best validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ffe6c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "539d9a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ec2c2180b648638bedcb7737eb5fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy at the end of epoch 0: 0.4908256880733945\n",
      "validation accuracy at the end of epoch 1: 0.588302752293578\n",
      "validation accuracy at the end of epoch 2: 0.6399082568807339\n",
      "validation accuracy at the end of epoch 3: 0.6674311926605505\n",
      "validation accuracy at the end of epoch 4: 0.6410550458715596\n",
      "validation accuracy at the end of epoch 5: 0.6892201834862385\n",
      "validation accuracy at the end of epoch 6: 0.6743119266055045\n",
      "validation accuracy at the end of epoch 7: 0.6548165137614679\n",
      "validation accuracy at the end of epoch 8: 0.6227064220183486\n",
      "validation accuracy at the end of epoch 9: 0.5928899082568807\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = 0\n",
    "train_dataset = SentimentData(train_data, train_label)\n",
    "val_dataset = SentimentData(val_data, val_label)\n",
    "clf = Convnet()\n",
    "train_loader = DataLoader(train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "for e in tqdm(range(epochs)):\n",
    "    for X, y in train_loader:\n",
    "        out = clf(X)\n",
    "        loss = criterion(out.reshape(-1), y.reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    clf.eval()\n",
    "    pred_class = []\n",
    "    true_class = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            out = clf(X)\n",
    "            #pred_class.extend(torch.a(out, dim=1).reshape(-1).numpy().tolist())\n",
    "            pred_class.extend(torch.round(torch.sigmoid(out)).reshape(-1).numpy().tolist())\n",
    "            true_class.extend(y.reshape(-1).numpy().tolist())\n",
    "    \n",
    "    score = accuracy_score(true_class, pred_class)\n",
    "    print(f'validation accuracy at the end of epoch {e}: {score}')\n",
    "    if score > max_accuracy:\n",
    "        max_accuracy = score\n",
    "        torch.save(clf.state_dict(), 'best_model.pt')\n",
    "    clf.train()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7074b7",
   "metadata": {},
   "source": [
    "4) Evaluate the model on the test set and report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad28ee0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_test = Convnet()\n",
    "clf_test.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4fbbde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test accuracy: 0.5930807248764415\n"
     ]
    }
   ],
   "source": [
    "clf_test.eval()\n",
    "test_dataset = SentimentData(test_data, test_label)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "pred_class = []\n",
    "true_class = []\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        out = clf(X)\n",
    "        pred_class.extend(torch.round(torch.sigmoid(out)).reshape(-1).numpy().tolist())\n",
    "        true_class.extend(y.reshape(-1).numpy().tolist())\n",
    "        \n",
    "score = accuracy_score(true_class, pred_class)\n",
    "print(f'Best test accuracy: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04726d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
